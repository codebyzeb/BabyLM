defaults: 
  - base_config
  - _self_
  - dataset: strict_small 
  - tokenizer: tuned
  - model: babyberta
  - objective_curriculum: base_mlm
  - data_curriculum: linear_data_split
  - vocabulary_curriculum: linear_pos_tags_and_token_ids
  - override hydra/launcher: submitit_slurm

dataset: 
  name: 'CamBabyTrainers/BabyLM'

experiment: 
  seed: 42 

data_preprocessing:
  include_punctuation: True
  max_input_length: 128

trainer: 
  batch_size: 128 # across 8 GPUs gives an effective batch size of 1024
  lr: 1e-4 # 1e-4 is used in fairseq; 1e-3 is default in huggingface
  num_warmup_steps: 1_500
  max_training_steps: 10_000
  eval_blimp: True
  eval_glue: True

  # here we specify SBATCH parameters
hydra:
  launcher:
    account: COMPUTERLAB-SL3-GPU
    partition: ampere
    name: ${hydra.job.name}
    nodes: 1
    tasks_per_node: 1
    gres: gpu:4