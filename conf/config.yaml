defaults: 
  - base_config
  - _self_
  - dataset: strict_small 
  - tokenizer: babyberta_tuned
  - model: babyberta_mlm
  - objective: mlm

dataset: 
  name: 'CamBabyTrainers/BabyLM'

experiment: 
  seed: 42 # life universe and everything 

data_preprocessing:
  include_punctuation: True
  max_input_length: 128

trainer: 
  batch_size: 16
  optimizer: 'adamw'
  scheduler: 'one-cycle-lr' 
  lr: 1e-4  # 1e-4 is used in fairseq (and performs better here), and 1e-3 is default in huggingface
  num_epochs: 1  # use 1 epoch to use dynamic masking
  num_warmup_steps: 24_000  # 24K used in Roberta-base
  weight_decay: 0.0