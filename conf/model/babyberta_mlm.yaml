name: 'roberta_pre_layer_norm_for_masked_lm'

load_from_checkpoint: False

# kwargs for model initialization 
num_hidden_layers: 8
num_attention_heads: 8
hidden_size: 256
intermediate_size: 1024
initializer_range: 0.02  # stdev of trunc normal for initializing all weights
layer_norm_eps: 1e-5  # 1e-5 default in fairseq (and slightly better performance), 1e-12 default in hgugingface, 