model: 'phueb/BabyBERTa-2'

load_from_checkpoint: False

num_layers: 8
num_attention_heads: 8
hidden_size: 256
intermediate_size: 1024
initializer_range: 0.02  # stdev of trunc normal for initializing all weights
layer_norm_eps: 1e-5  # 1e-5 default in fairseq (and slightly better performance), 1e-12 default in hgugingface, 